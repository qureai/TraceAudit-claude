In the data folder I have a .jsonl file which has about 50k entries and each line corresponds to one LLM trace. Each LLM trace will have multiple elements in them. So you have to find the latest element so that you would get all the messages or interactions. That latest element which will have all the messages. Generally there will be a system prompt which will describe the use_case of that particular llm trace. I want you to extract them and store them separately in a prompts folder. The idea is to analyze all these 50K traces and figure out how many unique such use_cases are there. So the overall idea is to build in a presentation using FastHTML code to showcase overall stats and also pick random traces for each use case to be shown. Code and also the UI on how an individual tray should look like can be found at this path: /Users/satishgolla/Desktop/work/TraceScope-V2/Z_OLD/LLM_Eval/ANNOT_PLATFORM. One thing which is not implemented in the code which I directed to you right for trace view is the model used for each interaction? Generally there will be a single model used for all the interactions, even multi-turn or even for tool calling or agent calling. But make sure that in the individual trace you include what is the model being used for that corresponding LLM call, tool call, or agent calling. While showcasing the individual traces for use_case, right? It is better if you show the trace on one side and corresponding system prompt on the other side. That is it, nothing else. And in the individual trace view, replace the system prompt with a placeholder and there should be a mapping to that system prompt and the placeholder. Ideal way to use that will be: make sure that placeholder would be the name of the file in which the content of the system prompt is being stored. We also have to do one more analysis recently we have implemented metadata tracking in these traces. I suspect the metadata can be found in keys such as x-portkey-metadata. I may be wrong about this but you check and see how many of them have metadata. Try different variants of the key which I have given and get stats on that also. Do the test run on 1K of those traces so that I can go through everything and give you any feedback if needed. The other thing which we also want to explore is how can we detect errors for each use case automatically. Example: If you see in the traces it is a multi-turn conversation then one of the methods which we were thinking was to if the user disagreed with the tray with the LLM output then we can say there is some error in the tray. But if it is a simple LLM interaction as in where it's only one-time conversation, how can you detect errors in those kinds of LLM Traces? Think about that also. Each use_case, how can we detect errors in them? Again this error detection shouldn't be in proxy for evaluation. Please keep that in mind. This is just so that this trace can be brought to human interaction saying that you should have a look at this trace. If you can build a nice beautiful useful interactive UI on top of all this analysis so that I can have all the analysis. Also include the time period in which all these traces were available. When it's comes to analysis, don't limit yourself to what I've just said. Be creative and try to get as many good analyses as you can, like maybe on the model usage. I don't know, be creative.